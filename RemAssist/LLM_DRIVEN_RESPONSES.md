# LLM-Driven Responses — Architectural Principle

**Created**: January 1, 2026 (Session 15)  
**Status**: ✅ IMPLEMENTED  
**Related Files**: `home-ai/soa1/api.py`, `soa-webui/templates/index.html`, `home-ai/soa1/prompts/orchestrator.md`

---

## The Problem

### Symptom
When users upload PDFs, they receive a terse, unhelpful response:
```
"Received: Apple Card Statement - September 2025.pdf. Analysis ready."
```

### Expected Behavior (per orchestrator.md)
```
"I've received Apple Card Statement - September 2025.pdf (7 pages, 1.2 MB).
From the headers, I can see this is your Apple Card statement for September 2025.

What would you like to do with this statement? You can:
- Get a quick summary of key details
- Look at specific sections (transactions, balances, etc.)
- Ask about specific transactions or patterns
- Request a spending analysis

Nothing happens automatically, so just tell me what you'd like to do!"
```

---

## Root Cause Analysis

### The Bug Chain

1. **Session 14 Fix**: Removed `consent_request` from `/upload-pdf` API response to eliminate specialist language ("I can involve the Finance Specialist...")

2. **Frontend Fallback**: `soa-webui/templates/index.html` line 287 has:
   ```javascript
   const msg = `Received: ${file.name}. ${data.consent_request ? data.consent_request.message : 'Analysis ready.'}`;
   ```

3. **Result**: When `consent_request` is undefined (correct), frontend falls back to hardcoded "Analysis ready." string

### The Architectural Flaw

The real problem isn't the missing `consent_request` — it's that **user-facing text was being generated by frontend code instead of the LLM**.

```
WRONG ARCHITECTURE:
┌─────────┐     ┌─────────┐     ┌──────────────┐
│ Upload  │ --> │   API   │ --> │   Frontend   │ --> User sees hardcoded string
│  PDF    │     │ (JSON)  │     │ (constructs  │
└─────────┘     └─────────┘     │   message)   │
                                └──────────────┘

CORRECT ARCHITECTURE:
┌─────────┐     ┌─────────┐     ┌─────────┐     ┌──────────────┐
│ Upload  │ --> │   API   │ --> │   LLM   │ --> │   Frontend   │ --> User sees LLM response
│  PDF    │     │         │     │ (agent) │     │ (displays    │
└─────────┘     └─────────┘     └─────────┘     │   as-is)     │
                                                └──────────────┘
```

---

## The Principle: LLM-Driven Responses

### Rule
**ALL user-facing text responses MUST be generated by the orchestrator LLM.**

### Applies To
| Interaction | Source | Example |
|-------------|--------|---------|
| Upload acknowledgment | LLM | "I've received your statement..." |
| Error messages | LLM | "I couldn't read that file because..." |
| Status updates | LLM | "Your analysis is complete..." |
| Consent requests | LLM | "Would you like me to analyze..." |
| Analysis results | LLM | "Here's what I found..." |

### Does NOT Apply To
| Interaction | Source | Reason |
|-------------|--------|--------|
| System errors (500) | Backend | LLM may be unavailable |
| Health check responses | Backend | Machine-to-machine |
| API metadata (doc_id, status codes) | Backend | Not user-facing text |

---

## The Fix

### Implementation Plan

#### 1. Modify `/upload-pdf` endpoint (`home-ai/soa1/api.py`)

After successful file processing, call the agent:

```python
# After saving document and creating job...

# Build document context for LLM
document_context = {
    "documents": [{
        "doc_id": doc_id,
        "filename": filename,
        "pages": pages_processed,
        "size_kb": file_size_bytes // 1024,
        "detected_type": detected_type,  # e.g., "apple_card", "bank_statement"
        "preview_text": text_preview[:500]
    }]
}

# Get LLM-generated response
agent = SOA1Agent()
agent_result = agent.ask(
    query=f"I just uploaded a document: {filename}",
    document_context=document_context
)

# Include in response
return {
    "status": "UPLOADED",
    "doc_id": doc_id,
    "filename": filename,
    "pages_processed": pages_processed,
    "agent_response": agent_result["answer"]  # <-- LLM-generated
}
```

#### 2. Update frontend (`soa-webui/templates/index.html`)

Remove hardcoded fallback, use API response directly:

```javascript
// BEFORE (wrong)
const msg = `Received: ${file.name}. ${data.consent_request ? data.consent_request.message : 'Analysis ready.'}`;

// AFTER (correct)
const msg = data.agent_response || `Received: ${file.name}`;  // Fallback only if API fails
```

#### 3. Ensure orchestrator.md has upload handling guidelines

The prompt should instruct the LLM how to respond to document uploads:
- Acknowledge receipt with specifics (filename, pages, size)
- Describe what was detected (statement type, institution)
- Offer clear options without assuming intent
- Emphasize nothing happens automatically

---

## Benefits

| Benefit | Description |
|---------|-------------|
| **Consistency** | All responses follow same communication style |
| **Maintainability** | Change behavior by editing prompt, not code |
| **Single Source of Truth** | `orchestrator.md` defines all behavior |
| **Client Agnostic** | Web, mobile, CLI all get same response |
| **Consent Compliance** | LLM enforces consent rules automatically |
| **Future Proof** | New features just need prompt updates |

---

## Anti-Patterns to Avoid

### ❌ Hardcoded User Messages in Frontend
```javascript
// BAD
const msg = "Your file has been uploaded successfully!";
```

### ❌ Hardcoded User Messages in Backend
```python
# BAD
return {"message": "Upload complete. Click analyze to continue."}
```

### ❌ Template Strings with Fallbacks
```javascript
// BAD - still has hardcoded fallback
const msg = data.message || "Something happened";
```

### ✅ Correct Approach
```python
# GOOD - API calls LLM
agent_response = agent.ask(query, context)
return {"agent_response": agent_response["answer"]}
```

```javascript
// GOOD - Frontend displays as-is
appendMessage('assistant', data.agent_response);
```

---

## Verification Checklist

After implementation, verify:

- [ ] Upload PDF → Response mentions filename, pages, size
- [ ] Upload PDF → Response offers specific options (summary, analysis, questions)
- [ ] Upload PDF → Response says "nothing happens automatically"
- [ ] Upload PDF → No mention of "specialist" or "phinance"
- [ ] Upload PDF → Response matches orchestrator.md tone/style
- [ ] Multiple uploads → Each gets contextual response
- [ ] Error case → LLM-generated error message (not stack trace)

---

## References

- **Orchestrator Prompt**: `home-ai/soa1/prompts/orchestrator.md`
- **Implementation Guide**: `RemAssist/IMPLEMENTATION_GUIDE.md`
- **Session 14 Summary**: Fixed specialist language exposure
- **Session 15 Summary**: This architectural fix

---

## Historical Context

### Why This Wasn't Caught Earlier

1. Original design had `consent_request` in API response with LLM-like message
2. Session 14 correctly removed it (exposed specialist language)
3. But the fix didn't replace it with proper LLM call
4. Frontend silently fell back to hardcoded string
5. Symptom only visible when testing actual upload flow

### Lesson Learned

When removing a feature that provides user-facing text, always verify:
1. What was generating that text?
2. What will generate it now?
3. Is there a hardcoded fallback that will activate?
