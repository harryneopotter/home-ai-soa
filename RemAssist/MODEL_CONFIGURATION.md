# ðŸ“‹ Model Configuration - Qwen 2.5 7B Instruct

## ðŸŽ¯ Current Model Configuration

### âœ… Active Model

**Model Name**: Qwen 2.5 7B Instruct
**Provider**: Ollama (Local)
**Status**: âœ… Loaded and Ready
**Endpoint**: `http://localhost:11434`

### ðŸ“Š Model Specifications

| Specification | Value |
|--------------|-------|
| **Model Family** | Qwen (Alibaba Cloud) |
| **Version** | 2.5 |
| **Size** | 7 Billion Parameters |
| **Type** | Instruct (Instruction-tuned) |
| **Quantization** | Optimized for local inference |
| **Context Window** | 32,768 tokens |
| **License** | Open-source (Apache 2.0) |

### ðŸŽ® Performance Characteristics

**Strengths**:
- âœ… Excellent instruction following
- âœ… Strong multi-turn conversation
- âœ… Good at complex reasoning
- âœ… Handles multiple languages well
- âœ… Efficient for local inference

**Use Cases**:
- âœ… Question answering
- âœ… Text generation
- âœ… Document analysis
- âœ… Summarization
- âœ… Code generation
- âœ… Multi-language support

### ðŸ”§ Configuration Settings

**Current SOA1 Configuration**:
```yaml
model:
  provider: "ollama"
  base_url: "http://localhost:11434"
  model_name: "qwen2.5:7b-instruct"
  temperature: 0.3
  max_tokens: 512
```

**Parameter Explanation**:
- **Temperature (0.3)**: Balanced creativity and focus
- **Max Tokens (512)**: Suitable for concise responses
- **Base URL**: Local Ollama server

### ðŸ“Š Available Models

**Currently Loaded**:
1. âœ… `qwen2.5:7b-instruct` (Primary)
2. âœ… `llama3.2:1b` (Backup)

**Available for Loading**:
- `qwen:7b`, `qwen:14b`, `qwen:72b`
- `llama2:7b`, `llama2:13b`, `llama2:70b`
- `mistral:7b`, `mistral:8x7b`
- `codellama:7b`, `codellama:13b`, `codellama:34b`

### ðŸŽ¯ Model Selection Rationale

**Why Qwen 2.5 7B Instruct?**

1. **Performance**: Excellent balance of quality and speed
2. **Size**: 7B fits well in GPU memory (RTX 3060)
3. **Capabilities**: Strong instruction following for agent tasks
4. **Efficiency**: Optimized for local inference
5. **Multilingual**: Good support for multiple languages

**Comparison with Alternatives**:

| Model | Size | Speed | Quality | Memory Usage |
|-------|------|-------|---------|---------------|
| Qwen 2.5 7B | 7B | â­â­â­â­ | â­â­â­â­ | Medium |
| Llama 3.2 1B | 1B | â­â­â­â­â­ | â­â­â­ | Low |
| Mistral 7B | 7B | â­â­â­â­ | â­â­â­â­ | Medium |
| Qwen 14B | 14B | â­â­â­ | â­â­â­â­â­ | High |

### ðŸŽ® GPU Utilization

**Current GPU Usage**:
- **Model Loaded**: Qwen 2.5 7B
- **VRAM Usage**: ~4.7GB
- **GPU**: RTX 3060 (12GB)
- **Available VRAM**: ~7.3GB free

**Performance Metrics**:
- **Tokens/sec**: ~20-30 tokens/sec
- **Response Time**: ~1-3 seconds for typical queries
- **Concurrency**: Single request (can be increased)

### ðŸ“‹ Model Management

**Load Different Model**:
```bash
ollama pull qwen:14b
ollama run qwen:14b
```

**Switch Models in SOA1**:
```yaml
# In config.yaml
model:
  model_name: "qwen:14b"  # Change to desired model
```

**Unload Model**:
```bash
# Ollama automatically manages memory
# Models are unloaded when not in use
```

### ðŸ” Model Testing

**Test Current Model**:
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b-instruct",
  "prompt": "Test response quality"
}'
```

**Benchmark Performance**:
```bash
# Test response speed
time curl -s http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b-instruct",
  "prompt": "Write a 100-word story about a robot."
}' > /dev/null
```

### ðŸŽ¯ Optimization Recommendations

**For Better Performance**:
1. **Increase GPU Batch Size**: Allow multiple concurrent requests
2. **Adjust Temperature**: Higher for creativity, lower for precision
3. **Use Streaming**: For better user experience
4. **Cache Responses**: For frequent similar queries

**For Different Use Cases**:
- **Coding Tasks**: Use `codellama:7b`
- **Creative Writing**: Use `qwen:7b` with higher temperature
- **Complex Reasoning**: Use `qwen:14b` (if VRAM allows)
- **Fast Responses**: Use `llama3.2:1b` (lower quality)

### ðŸ“š Model Documentation

**Official Resources**:
- Qwen GitHub: https://github.com/QwenLM/Qwen
- Qwen Documentation: https://qwenlm.github.io/
- Ollama Documentation: https://github.com/ollama/ollama

**Key Features**:
- âœ… Instruction-tuned for better responses
- âœ… Supports function calling
- âœ… Good at JSON output formatting
- âœ… Handles complex multi-turn conversations
- âœ… Strong in mathematics and logic

### ðŸŽ‰ Current Status

**Model**: âœ… Qwen 2.5 7B Instruct
**Status**: âœ… Loaded and Ready
**Performance**: âœ… Optimal for SOA1 tasks
**Configuration**: âœ… Properly configured
**Integration**: âœ… Working with PDF demo

The Qwen 2.5 7B Instruct model is perfectly configured for our PDF demo and provides an excellent balance of quality, speed, and efficiency for local inference tasks!
